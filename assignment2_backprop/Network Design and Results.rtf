{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf400
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 2. \
\
Initial Weights: random values between 0 and 1\
\
Node output function: \
Sigmoid, used because it is differentiable everywhere which is a requirement for backpropagation\
\
Learning rate: 0.7\
Common learning rates are between 0 and 1, so I tried various values until I found the best one\
\
Termination Criteria:\
Did not use terminating criteria, used trial and error to figure out how many epochs were required\
\
Number of layers and nodes:\
For a simple problem like this one, it was evident that only one hidden layer would be needed. \
My input layer has 4 nodes (for 4 parameters), my hidden layer also has 4, since increasing it did not change much, and my output layer has 3. Using 3 output nodes, each category can be converted to binary representation.\
\
Momentum: 0.4\
For momentum, lower values would return worse error since the network would get stuck in local minimas. This value proved to be the best I could find.\
\
Data pre-processing: \
I produced a correlation matrix to determine which parameters related most to quality and by how much, as well as how much these parameters related to others. Using this, I chose the 4 parameters which correlated most with quality. I also normalized the data so that different parameters would not affect the changes made by more or else.\
\
Percentage data split:\
Training: 70%\
Validation: 15%\
Training: 15%\
Used a regular data split which Matlab uses as well. Did not find that using more data for training had much of an impact on error or accuracy.\
\
3. \
\
Precision: My network is able to greatly reduce MSE (as seen in error plot) but could not converge. Therefore I only got an accuracy of 5%.\
 \
Final weights:\
Hidden:\
[[ 116.77351352  -20.7008805   -90.04577363  -68.2736032   -15.58808497]\
 [ -90.1415908    18.53926619   69.35521419   52.74259744   12.97689607]\
 [ -57.0104296    11.57355335   43.11944377   33.0634646     8.62150994]\
 [ -93.43983188   18.57615442   72.00979794   55.18272793   13.18645041]\
 [ -83.402513     16.75866587   64.12431166   49.06581178   11.46341352]]\
\
Output:\
[[-13.62842294   8.98671008   6.35753516   9.88231615   8.65078978]\
 [-16.27430556  10.90587058   6.96769239  11.88124592  10.20467587]\
 [-20.82464846  12.79093008   9.02610961  13.47238876  12.08993472]]\
}